{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea3dc0f-ae60-47ee-8284-d1963b2e2626",
   "metadata": {},
   "source": [
    "## MLP - Multi-Layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fbf788-7454-4cc4-9c78-53cc936c86f9",
   "metadata": {},
   "source": [
    "### What is perceptron and multi layer perceptron\n",
    "A perceptron is one of the simplest types of artificial neural networks and is the fundamental building block of more complex neural network architectures. It acts like a single neuron in a neural network. The perceptron receives input signals, processes them through weights and a bias, and then produces an output. \n",
    "\n",
    "### Multi-layer Perceptron (MLP)\n",
    "A multi-layer perceptron, or MLP, consists of multiple layers of neurons: an input layer, one or more hidden layers, and an output layer. Each layer is fully connected to the next one. MLPs can solve more complex problems than a single perceptron because the multiple layers allow the network to learn more intricate patterns.\n",
    "\n",
    "##### Structure of MLP:\n",
    "\n",
    "Input Layer: Takes the initial data.\n",
    "\n",
    "Hidden Layers: Intermediate layers that perform complex computations through activation functions. These can be one or more layers.\n",
    "\n",
    "Output Layer: Produces the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20332a71-e624-441f-b2fd-64181c9f3389",
   "metadata": {},
   "source": [
    "![MLP structure](Multi-Layer-Perceptron-MLP-diagram-with-four-hidden-layers-and-a-collection-of-single.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c749d9-648f-4f68-8cbe-46cf496c3207",
   "metadata": {},
   "source": [
    "The connections between neurons are adjusted through a process called backpropagation during the training phase, which allows the network to learn from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292c5bb1-b2e2-4087-8de0-deaa464ddf01",
   "metadata": {},
   "source": [
    "### Key Assumptions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c451ae9b-49b5-4e74-a23b-0a07c39a286a",
   "metadata": {},
   "source": [
    "1. Data Preprocessing:\n",
    "\n",
    "- Normalization/Standardization: Input data should be normalized or standardized so that features have a similar scale, which helps the network learn more effectively.\n",
    "- Quality and Quantity of Data: High-quality data without noise and a sufficient quantity of data are essential for training an MLP to generalize well.\n",
    "\n",
    "2. Independence of Data:\n",
    "\n",
    "- Independent and Identically Distributed (i.i.d.): The data samples are assumed to be i.i.d., meaning each data point is drawn from the same probability distribution and is independent of other data points.\n",
    "\n",
    "3. Architecture Design:\n",
    "\n",
    "- Layer and Neuron Count: The number of layers and neurons in each layer should be chosen carefully. Too few may lead to underfitting, while too many may lead to overfitting.\n",
    "- Activation Functions: Suitable activation functions (e.g., ReLU, sigmoid, tanh) should be chosen for neurons to introduce non-linearity and enable the network to learn complex patterns.\n",
    "\n",
    "4. Learning Process:\n",
    "\n",
    "- Optimization Algorithms: Gradient-based optimization algorithms, such as SGD or Adam, are assumed to be effective for training the network.\n",
    "- Learning Rate: The learning rate should be appropriately set to ensure convergence without causing oscillations or slow learning.\n",
    "\n",
    "5. Loss Function:\n",
    "\n",
    "- Appropriate Loss Function: The loss function used should be suitable for the specific task (e.g., mean squared error for regression, cross-entropy loss for classification).\n",
    "\n",
    "6. Overfitting and Regularization:\n",
    "\n",
    "- Regularization Techniques: Techniques such as L2 regularization, dropout, or early stopping should be used to prevent overfitting and improve generalization.\n",
    "- Cross-Validation: Cross-validation techniques can help assess model performance and avoid overfitting.\n",
    "\n",
    "7. Computational Resources:\n",
    "\n",
    "Adequate Hardware: Sufficient computational resources (CPU/GPU) are assumed to be available for efficient training and evaluation of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba65dc4-1a04-477c-bbed-a03208d1b5f6",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "- Universal Approximation: MLPs can approximate any continuous function given sufficient neurons and layers, making them versatile for a wide range of tasks.\n",
    "\n",
    "- Capability to Handle Non-linearity: By using activation functions like ReLU, sigmoid, or tanh, MLPs can model complex, non-linear relationships in data.\n",
    "\n",
    "- Application Flexibility: MLPs can be used for various tasks, including classification, regression, and even some unsupervised learning problems.\n",
    "\n",
    "- Adaptability: MLPs can be adapted and scaled to more complex networks like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) for specialized tasks such as image and sequence processing.\n",
    "\n",
    "- Feature Learning: MLPs can automatically learn and extract useful features from the input data, reducing the need for manual feature engineering.\n",
    "\n",
    "### Disadvantages\n",
    "- Computationally Intensive: Training MLPs can be computationally expensive, requiring significant processing power and time, especially for large datasets and deep architectures.\n",
    "\n",
    "- Sensitive to Hyperparameters: MLPs require careful tuning of hyperparameters (e.g., learning rate, number of layers, and neurons) to perform optimally, which can be a challenging and time-consuming process.\n",
    "\n",
    "- Prone to Overfitting: Without proper regularization techniques, MLPs can easily overfit to the training data, resulting in poor generalization to unseen data.\n",
    "\n",
    "- Black Box Nature: The internal workings of MLPs can be opaque, making it difficult to interpret how the model makes decisions, which is a challenge for applications requiring high interpretability.\n",
    "\n",
    "- Dependence on Data Quality: The performance of MLPs heavily relies on the quality and quantity of the training data. Noisy or insufficient data can lead to suboptimal model performance.\n",
    "\n",
    "- Requires Large Datasets: MLPs typically perform best with large datasets, which might not always be available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfce967-d23d-4997-96e9-a7dc4846a595",
   "metadata": {},
   "source": [
    "### How does a Multi Layer perceptron work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee4104b-9d1d-4f51-a34e-5b726293c7fe",
   "metadata": {},
   "source": [
    "Interactive tool - https://chokkan.github.io/deeplearning/demo-mlp.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4752f4-728e-400d-86a6-08e8f1551872",
   "metadata": {},
   "source": [
    "Another interactive tool - https://perceptrondemo.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcfa1ed-f292-43a9-9061-ceec90a3f8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
