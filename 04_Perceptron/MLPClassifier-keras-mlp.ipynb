{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-12-05T07:21:40.344979Z",
     "iopub.status.busy": "2021-12-05T07:21:40.344625Z",
     "iopub.status.idle": "2021-12-05T07:21:40.374684Z",
     "shell.execute_reply": "2021-12-05T07:21:40.373938Z",
     "shell.execute_reply.started": "2021-12-05T07:21:40.344898Z"
    }
   },
   "source": [
    "<div style=\"color:#D81F26;\n",
    "           display:fill;\n",
    "           border-style: solid;\n",
    "           border-color:#C1C1C1;\n",
    "           font-size:14px;\n",
    "           font-family:Calibri;\n",
    "           background-color:#373737;\">\n",
    "<h2 style=\"text-align: center;\n",
    "           padding: 10px;\n",
    "           color:#FFFFFF;\">\n",
    "======= Keras MLP =======\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for submission to the Playground of Dec 2021. A simple Keras sequential neural network has been usedk. Different configuration of kernel initizlizers and activation functions have been experimented for performance comparison. No. of neurons and layers will be experimented later. \n",
    "\n",
    "Some codes are referred and copied from other notebooks.\n",
    "​\n",
    "## Summary\n",
    "* Read the training and testing data sets with memory optimization  \n",
    "* Perform EDA to examine the data and decide which features can be dropped\n",
    "* Initialize  a 3-layer Keras sequential neural network with following model configuration\n",
    "    * Kernel initializer of \"lecun_normal\" and \"he_normal\"\n",
    "    * Activation function of \"selu\", \"relu\", \"elu\" and \"swish\"\n",
    "* Get the optimal models for cross-validation and data submission\n",
    "​\n",
    "\n",
    "## Reference\n",
    "* \"TPS - Oct 2021 Model with Memory reduced\" by S T MOHAMMED @stmohd (URL: https://www.kaggle.com/stmohd/tps-oct-2021-model-with-memory-reduced)\n",
    "* \"TPS - Nov 2021 - NN\" by ŞAFAK TÜRKELI @sfktrkl (URL: [https://www.kaggle.com/sfktrkl/tps-nov-2021-nn](https://www.kaggle.com/sfktrkl/tps-nov-2021-nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic library\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import stats\n",
    "\n",
    "# Scaler\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "\n",
    "# Cross-Validation\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 259.40 Mb (84.8% reduction)\n",
      "Mem. usage decreased to 63.90 Mb (84.8% reduction)\n",
      "Memory reduced\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('./tabular-playground-series-dec-2021/train.csv')\n",
    "df_test = pd.read_csv('./tabular-playground-series-dec-2021/test.csv')\n",
    "\n",
    "\n",
    "## from: https://freedium.cfd/https://medium.com/towards-data-science/how-to-work-with-million-row-datasets-like-a-pro-76fb5c381cdd#:~:text=Today%2C%20I%20am%20here%20to%20share%20the%20concepts,natural%20as%20working%20with%20the%20Iris%20or%20Titanic.\n",
    "def reduce_memory_usage(df, verbose=True):\n",
    "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df\n",
    "\n",
    "df_train = reduce_memory_usage(df_train, verbose=True)\n",
    "df_test = reduce_memory_usage(df_test, verbose=True)\n",
    "print('Memory reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000000 entries, 0 to 3999999\n",
      "Data columns (total 56 columns):\n",
      " #   Column                              Dtype\n",
      "---  ------                              -----\n",
      " 0   Id                                  int32\n",
      " 1   Elevation                           int16\n",
      " 2   Aspect                              int16\n",
      " 3   Slope                               int8 \n",
      " 4   Horizontal_Distance_To_Hydrology    int16\n",
      " 5   Vertical_Distance_To_Hydrology      int16\n",
      " 6   Horizontal_Distance_To_Roadways     int16\n",
      " 7   Hillshade_9am                       int16\n",
      " 8   Hillshade_Noon                      int16\n",
      " 9   Hillshade_3pm                       int16\n",
      " 10  Horizontal_Distance_To_Fire_Points  int16\n",
      " 11  Wilderness_Area1                    int8 \n",
      " 12  Wilderness_Area2                    int8 \n",
      " 13  Wilderness_Area3                    int8 \n",
      " 14  Wilderness_Area4                    int8 \n",
      " 15  Soil_Type1                          int8 \n",
      " 16  Soil_Type2                          int8 \n",
      " 17  Soil_Type3                          int8 \n",
      " 18  Soil_Type4                          int8 \n",
      " 19  Soil_Type5                          int8 \n",
      " 20  Soil_Type6                          int8 \n",
      " 21  Soil_Type7                          int8 \n",
      " 22  Soil_Type8                          int8 \n",
      " 23  Soil_Type9                          int8 \n",
      " 24  Soil_Type10                         int8 \n",
      " 25  Soil_Type11                         int8 \n",
      " 26  Soil_Type12                         int8 \n",
      " 27  Soil_Type13                         int8 \n",
      " 28  Soil_Type14                         int8 \n",
      " 29  Soil_Type15                         int8 \n",
      " 30  Soil_Type16                         int8 \n",
      " 31  Soil_Type17                         int8 \n",
      " 32  Soil_Type18                         int8 \n",
      " 33  Soil_Type19                         int8 \n",
      " 34  Soil_Type20                         int8 \n",
      " 35  Soil_Type21                         int8 \n",
      " 36  Soil_Type22                         int8 \n",
      " 37  Soil_Type23                         int8 \n",
      " 38  Soil_Type24                         int8 \n",
      " 39  Soil_Type25                         int8 \n",
      " 40  Soil_Type26                         int8 \n",
      " 41  Soil_Type27                         int8 \n",
      " 42  Soil_Type28                         int8 \n",
      " 43  Soil_Type29                         int8 \n",
      " 44  Soil_Type30                         int8 \n",
      " 45  Soil_Type31                         int8 \n",
      " 46  Soil_Type32                         int8 \n",
      " 47  Soil_Type33                         int8 \n",
      " 48  Soil_Type34                         int8 \n",
      " 49  Soil_Type35                         int8 \n",
      " 50  Soil_Type36                         int8 \n",
      " 51  Soil_Type37                         int8 \n",
      " 52  Soil_Type38                         int8 \n",
      " 53  Soil_Type39                         int8 \n",
      " 54  Soil_Type40                         int8 \n",
      " 55  Cover_Type                          int8 \n",
      "dtypes: int16(9), int32(1), int8(46)\n",
      "memory usage: 259.4 MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.hist(bins=25, figsize = (20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics of each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>Wilderness_Area1</th>\n",
       "      <th>Wilderness_Area2</th>\n",
       "      <th>Wilderness_Area3</th>\n",
       "      <th>Wilderness_Area4</th>\n",
       "      <th>Soil_Type1</th>\n",
       "      <th>Soil_Type2</th>\n",
       "      <th>Soil_Type3</th>\n",
       "      <th>Soil_Type4</th>\n",
       "      <th>Soil_Type5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.000000e+06</td>\n",
       "      <td>4.000000e+06</td>\n",
       "      <td>4.000000e+06</td>\n",
       "      <td>4.000000e+06</td>\n",
       "      <td>4.000000e+06</td>\n",
       "      <td>4.000000e+06</td>\n",
       "      <td>4.000000e+06</td>\n",
       "      <td>4.000000e+06</td>\n",
       "      <td>4.000000e+06</td>\n",
       "      <td>4.000000e+06</td>\n",
       "      <td>4.000000e+06</td>\n",
       "      <td>4.000000e+06</td>\n",
       "      <td>4.000000e+06</td>\n",
       "      <td>4.000000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.766642e+03</td>\n",
       "      <td>2.118375e+02</td>\n",
       "      <td>2.210614e+02</td>\n",
       "      <td>1.408109e+02</td>\n",
       "      <td>1.581407e+03</td>\n",
       "      <td>2.611930e-01</td>\n",
       "      <td>4.166100e-02</td>\n",
       "      <td>6.535732e-01</td>\n",
       "      <td>2.181900e-02</td>\n",
       "      <td>1.684150e-02</td>\n",
       "      <td>3.089600e-02</td>\n",
       "      <td>4.275500e-03</td>\n",
       "      <td>3.791275e-02</td>\n",
       "      <td>1.571525e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.315610e+03</td>\n",
       "      <td>3.075996e+01</td>\n",
       "      <td>2.223134e+01</td>\n",
       "      <td>4.369864e+01</td>\n",
       "      <td>1.127616e+03</td>\n",
       "      <td>4.392849e-01</td>\n",
       "      <td>1.998133e-01</td>\n",
       "      <td>4.758312e-01</td>\n",
       "      <td>1.460922e-01</td>\n",
       "      <td>1.286774e-01</td>\n",
       "      <td>1.730360e-01</td>\n",
       "      <td>6.524738e-02</td>\n",
       "      <td>1.909853e-01</td>\n",
       "      <td>1.243716e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.870000e+02</td>\n",
       "      <td>-4.000000e+00</td>\n",
       "      <td>4.900000e+01</td>\n",
       "      <td>-5.300000e+01</td>\n",
       "      <td>-2.770000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.220000e+02</td>\n",
       "      <td>1.980000e+02</td>\n",
       "      <td>2.100000e+02</td>\n",
       "      <td>1.150000e+02</td>\n",
       "      <td>7.810000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.436000e+03</td>\n",
       "      <td>2.180000e+02</td>\n",
       "      <td>2.240000e+02</td>\n",
       "      <td>1.420000e+02</td>\n",
       "      <td>1.361000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.365000e+03</td>\n",
       "      <td>2.330000e+02</td>\n",
       "      <td>2.370000e+02</td>\n",
       "      <td>1.690000e+02</td>\n",
       "      <td>2.084000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.666000e+03</td>\n",
       "      <td>3.010000e+02</td>\n",
       "      <td>2.790000e+02</td>\n",
       "      <td>2.720000e+02</td>\n",
       "      <td>8.075000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Horizontal_Distance_To_Roadways  Hillshade_9am  Hillshade_Noon  \\\n",
       "count                     4.000000e+06   4.000000e+06    4.000000e+06   \n",
       "mean                      1.766642e+03   2.118375e+02    2.210614e+02   \n",
       "std                       1.315610e+03   3.075996e+01    2.223134e+01   \n",
       "min                      -2.870000e+02  -4.000000e+00    4.900000e+01   \n",
       "25%                       8.220000e+02   1.980000e+02    2.100000e+02   \n",
       "50%                       1.436000e+03   2.180000e+02    2.240000e+02   \n",
       "75%                       2.365000e+03   2.330000e+02    2.370000e+02   \n",
       "max                       7.666000e+03   3.010000e+02    2.790000e+02   \n",
       "\n",
       "       Hillshade_3pm  Horizontal_Distance_To_Fire_Points  Wilderness_Area1  \\\n",
       "count   4.000000e+06                        4.000000e+06      4.000000e+06   \n",
       "mean    1.408109e+02                        1.581407e+03      2.611930e-01   \n",
       "std     4.369864e+01                        1.127616e+03      4.392849e-01   \n",
       "min    -5.300000e+01                       -2.770000e+02      0.000000e+00   \n",
       "25%     1.150000e+02                        7.810000e+02      0.000000e+00   \n",
       "50%     1.420000e+02                        1.361000e+03      0.000000e+00   \n",
       "75%     1.690000e+02                        2.084000e+03      1.000000e+00   \n",
       "max     2.720000e+02                        8.075000e+03      1.000000e+00   \n",
       "\n",
       "       Wilderness_Area2  Wilderness_Area3  Wilderness_Area4    Soil_Type1  \\\n",
       "count      4.000000e+06      4.000000e+06      4.000000e+06  4.000000e+06   \n",
       "mean       4.166100e-02      6.535732e-01      2.181900e-02  1.684150e-02   \n",
       "std        1.998133e-01      4.758312e-01      1.460922e-01  1.286774e-01   \n",
       "min        0.000000e+00      0.000000e+00      0.000000e+00  0.000000e+00   \n",
       "25%        0.000000e+00      0.000000e+00      0.000000e+00  0.000000e+00   \n",
       "50%        0.000000e+00      1.000000e+00      0.000000e+00  0.000000e+00   \n",
       "75%        0.000000e+00      1.000000e+00      0.000000e+00  0.000000e+00   \n",
       "max        1.000000e+00      1.000000e+00      1.000000e+00  1.000000e+00   \n",
       "\n",
       "         Soil_Type2    Soil_Type3    Soil_Type4    Soil_Type5  \n",
       "count  4.000000e+06  4.000000e+06  4.000000e+06  4.000000e+06  \n",
       "mean   3.089600e-02  4.275500e-03  3.791275e-02  1.571525e-02  \n",
       "std    1.730360e-01  6.524738e-02  1.909853e-01  1.243716e-01  \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[:,6:20].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the describe() functions above, the variable Soil_Type7 and Soil_Type15 can be removed due to same value for all instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cover_Type\n",
       "2    2262087\n",
       "1    1468136\n",
       "3     195712\n",
       "7      62261\n",
       "6      11426\n",
       "4        377\n",
       "5          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Cover_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the value_counts() function above, there is only 1 instance for class 5.  Thus, treat it as outlier and remove it from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = df_train[df_train['Cover_Type'] != 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train data without the target and ids\n",
    "X = df_train.iloc[:, 1:-1].copy()\n",
    "X.drop(columns=['Soil_Type7', 'Soil_Type15'], inplace=True)\n",
    "\n",
    "# Get the target\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df_train['Cover_Type'])\n",
    "\n",
    "# Create test X, drop ids.\n",
    "test_X = df_test.iloc[:, 1:].copy()\n",
    "test_X.drop(columns=['Soil_Type7', 'Soil_Type15'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training History\n",
    "\n",
    "The following configuration had been tried.\n",
    "\n",
    "    # layers.Dense(64, activation='selu')      \n",
    "    # layers.Dense(64, kernel_initializer='lecun_normal', activation='selu')    \n",
    "    # layers.Dense(64, kernel_initializer='he_normal', activation='relu')      \n",
    "    # layers.Dense(64, kernel_initializer='he_normal', activation='elu')\n",
    "    # layers.Dense(64, activation='swish')\n",
    "\n",
    "```\n",
    "Configuration 1: \n",
    "    callbacks: EarlyStopping min_delta=0.001, patience=20\n",
    "    EPOCHS: 50, BATCH_SIZE: 512, N_SPLITS: 5\n",
    "    150 neurons, activation='selu', BatchNormalization\n",
    "    70 neurons, activation='selu', BatchNormalization\n",
    "    40 neurons, activation='selu', BatchNormalization\n",
    "    7 neurons, activation='softmax'\n",
    "    \n",
    "    Overall Mean AUC:  0.9608532499999999\n",
    "```\n",
    "```\n",
    "Configuration 2: \n",
    "    callbacks: EarlyStopping min_delta=0.001, patience=20\n",
    "    EPOCHS: 50, BATCH_SIZE: 512, N_SPLITS: 5\n",
    "    150 neurons, activation='selu', kernel_initializer='lecun_normal', BatchNormalization\n",
    "    70 neurons, activation='selu', kernel_initializer='lecun_normal', BatchNormalization\n",
    "    40 neurons, activation='selu', kernel_initializer='lecun_normal', BatchNormalization\n",
    "    7 neurons, activation='softmax'\n",
    "    \n",
    "    Overall Mean AUC:  0.96171275\n",
    "```\n",
    "```\n",
    "Configuration 3: \n",
    "    callbacks: EarlyStopping min_delta=0.001, patience=20\n",
    "    EPOCHS: 50, BATCH_SIZE: 512, N_SPLITS: 5\n",
    "    150 neurons, activation='relu', kernel_initializer='he_normal', BatchNormalization\n",
    "    70 neurons, activation='relu', kernel_initializer='he_normal', BatchNormalization\n",
    "    40 neurons, activation='relu', kernel_initializer='he_normal', BatchNormalization\n",
    "    7 neurons, activation='softmax'\n",
    "    \n",
    "    Overall Mean AUC: 0.9615895\n",
    "```\n",
    "```\n",
    "Configuration 4: \n",
    "    callbacks: EarlyStopping min_delta=0.001, patience=20\n",
    "    EPOCHS: 50, BATCH_SIZE: 512, N_SPLITS: 5\n",
    "    150 neurons, activation='swish', BatchNormalization\n",
    "    70 neurons, activation='swish', BatchNormalization\n",
    "    40 neurons, activation='swish', BatchNormalization\n",
    "    7 neurons, activation='softmax'\n",
    "    \n",
    "    Overall Mean AUC: 0.9612097500000001\n",
    "```    \n",
    "```\n",
    "Configuration 5: \n",
    "    callbacks: EarlyStopping min_delta=0.001, patience=20\n",
    "    EPOCHS: 50, BATCH_SIZE: 512, N_SPLITS: 5\n",
    "    150 neurons, activation='elu', kernel_initializer='he_normal', BatchNormalization\n",
    "    70 neurons, activation='elu', kernel_initializer='he_normal', BatchNormalization\n",
    "    40 neurons, activation='elu', kernel_initializer='he_normal', BatchNormalization\n",
    "    7 neurons, activation='softmax'\n",
    "    \n",
    "    Overall Mean AUC: 0.96141325\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model -  Keras NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_acc\", \n",
    "    min_delta=0.001,           # Minimium amount of change to count as an improvement\n",
    "    patience=5,               # How many epochs to wait before stopping\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "# Factor by which the learning rate will be reduced        \n",
    "#    factor=0.1,                \n",
    "    factor=0.2,                # Factor by which the learning rate will be reduced\n",
    "    patience=5,                # Number of epochs with no improvement\n",
    "    min_lr=0.001)              # Lower bound on the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALLBACKS = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with those configurations...\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 4096\n",
    "N_SPLITS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w.X+b = y\n",
    "# y = mx+c\n",
    "\n",
    "# w = w - alpha.delta(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadiq\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# model = keras.Sequential()\n",
    "# model.add(tf.keras.layers.Dense(600, kernel_initializer='lecun_normal', activation='selu'))\n",
    "# model.add(tf.keras.layers.Dense(300, kernel_initializer='lecun_normal', activation='selu'))\n",
    "# model.add(tf.keras.layers.Dense(150, kernel_initializer='lecun_normal', activation='selu'))\n",
    "# model.add(tf.keras.layers.Dense(75, kernel_initializer='lecun_normal', activation='selu'))\n",
    "# model.add(tf.keras.layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(600,  activation='selu', kernel_initializer='lecun_normal', input_shape=[X.shape[1]]),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(300,  activation='selu', kernel_initializer='lecun_normal'),   \n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(150,  activation='selu', kernel_initializer='lecun_normal'),    \n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(75,  activation='selu', kernel_initializer='lecun_normal'),    \n",
    "    layers.BatchNormalization(),    \n",
    "    # For a binary classification function use sigmoid\n",
    "    layers.Dense(7, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadiq\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=20.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step\n",
      "Kfold = 0, Acc = 0.96073\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
      "Kfold = 1, Acc = 0.96066\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "Kfold = 2, Acc = 0.96125\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
      "Kfold = 3, Acc = 0.96236\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "Kfold = 4, Acc = 0.96164\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 32ms/step\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
      "Kfold = 5, Acc = 0.96225\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 31ms/step\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "Kfold = 6, Acc = 0.96296\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
      "Kfold = 7, Acc = 0.96261\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "Kfold = 8, Acc = 0.96311\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 31ms/step\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
      "Kfold = 9, Acc = 0.96275\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 31ms/step\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "Kfold = 10, Acc = 0.96284\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step\n",
      "Kfold = 11, Acc = 0.96274\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 30ms/step\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
      "Kfold = 12, Acc = 0.96298\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "Kfold = 13, Acc = 0.96392\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "Kfold = 14, Acc = 0.96330\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "Kfold = 15, Acc = 0.96271\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "Kfold = 16, Acc = 0.96376\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "Kfold = 17, Acc = 0.96354\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "Kfold = 18, Acc = 0.96418\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "Kfold = 19, Acc = 0.96400\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step\n",
      "Overall Mean AUC:  0.9627157499999998\n"
     ]
    }
   ],
   "source": [
    "# fold = 0\n",
    "#test_predictions = np.zeros(test_X.shape[0])\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, random_state=48, shuffle=True)\n",
    "#scores = {fold:None for fold in range(skf.n_splits)}\n",
    "model_list = []\n",
    "accuracy_list  = []\n",
    "fold_list  = []\n",
    "test_predictions = []\n",
    "\n",
    "for fold,(train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "# for train_idx, test_idx in skf.split(X, y):\n",
    "    train_X, val_X = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    train_y, val_y = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Train\n",
    "    preproc = StandardScaler() # I tried QuantileTransformer, but StandardScaler seems to be better by 0.005\n",
    "    train_X = preproc.fit_transform(train_X)\n",
    "    val_X = preproc.transform(val_X)\n",
    "    \n",
    "    # Test\n",
    "    pred_X = preproc.transform(test_X)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_X, train_y,\n",
    "        validation_data=(val_X, val_y), \n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[reduce_lr, early_stopping],        # Put your callbacks in a list\n",
    "        verbose=0)                  # Turn off training log\n",
    "\n",
    "    val_y_pred = model.predict(val_X, batch_size=BATCH_SIZE)\n",
    "    val_y_pred = np.argmax(val_y_pred, axis=1)\n",
    "\n",
    "    # Evaluation\n",
    "    accuracy = accuracy_score(val_y, val_y_pred)\n",
    "    \n",
    "    accuracy_list.append(accuracy)\n",
    "    model_list.append(history)\n",
    "    fold_list.append(fold)\n",
    "    \n",
    "    print('Kfold = {}, Acc = {:0.5f}'.format(fold, accuracy))\n",
    "    \n",
    "    \n",
    "    \n",
    "    pred_y = model.predict(pred_X, batch_size=BATCH_SIZE)\n",
    "    pred_y = np.argmax(pred_y, axis=1)\n",
    "    test_predictions.append(pred_y)\n",
    "    \n",
    "    \n",
    "#overall_auc = [np.max(scores[fold]['val_acc']) for fold in range(skf.n_splits)]\n",
    "print('Overall Mean AUC: ', np.mean(accuracy_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
